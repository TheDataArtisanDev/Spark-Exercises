{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start a Spark session\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext(\"local\", \"RDDFilterExample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Spark is a unified analytics engine.', 'It provides high-level APIs in Java, Scala, Python, and R.', 'PySpark is the Python API for Spark.', 'It allows working with RDDs and DataFrames in Python.', 'You can perform data processing and machine learning in parallel.']\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkFiles\n",
    "\n",
    "# Add the file to Spark\n",
    "sc.addFile(r\"Spark-Exercises\\Datasets\\01_sample_text.txt\") \n",
    "# You need to \"Replace\" with the actual file path\n",
    "\n",
    "# Load the file using SparkFiles.get() to access the file from Spark's distributed file system\n",
    "file_path = SparkFiles.get(\"01_sample_text.txt\")\n",
    "rdd = sc.textFile(f\"file:///{file_path}\")\n",
    "\n",
    "# Perform an action, like collecting the data\n",
    "data = rdd.collect()\n",
    "\n",
    "# Output the result\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
