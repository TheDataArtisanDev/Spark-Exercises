{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem Statement:\n",
    "\n",
    "You want to fill the NULL values in the brand column by using product_id as a mapping key. For instance, if there are rows where brand is NULL and the product_id is the same as another row where the brand is not NULL, you want to fill the missing brand based on the product_id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"SparkBackFilling\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input:\n",
    "\n",
    "# +-------+-----------+\n",
    "# | brand | product_id|\n",
    "# +-------+-----------+\n",
    "# | A     | 1234      |\n",
    "# | B     | 5678      |\n",
    "# | NULL  | 5678      |\n",
    "# | C     | 9101      |\n",
    "# | NULL  | 1234      |\n",
    "# | NULL  | 9101      |\n",
    "# | D     | 1112      |\n",
    "# +-------+-----------+\n",
    "\n",
    "# output:\n",
    "\n",
    "# +-------+-----------+\n",
    "# | brand | product_id|\n",
    "# +-------+-----------+\n",
    "# | A     | 1234      |\n",
    "# | B     | 5678      |\n",
    "# | B     | 5678      |\n",
    "# | C     | 9101      |\n",
    "# | A     | 1234      |\n",
    "# | C     | 9101      |\n",
    "# | D     | 1112      |\n",
    "# +-------+-----------+\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n",
      "|brand|product_id|\n",
      "+-----+----------+\n",
      "|A    |1234      |\n",
      "|B    |5678      |\n",
      "|NULL |5678      |\n",
      "|C    |9101      |\n",
      "|NULL |1234      |\n",
      "|NULL |9101      |\n",
      "|D    |1112      |\n",
      "+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"BrandProductFilling\").getOrCreate()\n",
    "\n",
    "# Sample data with nulls in the brand column\n",
    "data = [\n",
    "    ('A', 1234),\n",
    "    ('B', 5678),\n",
    "    (None, 5678),\n",
    "    ('C', 9101),\n",
    "    (None, 1234),\n",
    "    (None, 9101),\n",
    "    ('D', 1112)\n",
    "]\n",
    "\n",
    "# Define schema and create DataFrame\n",
    "columns = ['brand', 'product_id']\n",
    "df = spark.createDataFrame(data, schema=columns)\n",
    "\n",
    "df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|product_id|brand|\n",
      "+----------+-----+\n",
      "|      1234|    A|\n",
      "|      5678|    B|\n",
      "|      5678|    B|\n",
      "|      9101|    C|\n",
      "|      1234|    A|\n",
      "|      9101|    C|\n",
      "|      1112|    D|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as fx\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "window_spec = Window.partitionBy('product_id')\n",
    "\n",
    "df_not_missing = df.filter(fx.col('brand').isNotNull())\n",
    "\n",
    "\n",
    "# Perform a left join to map missing brands based on product_id\n",
    "df_filled = df.alias('df1').join(\n",
    "    df_not_missing.alias('df2'),\n",
    "    on='product_id',\n",
    "    how='left'\n",
    ").select(\n",
    "    'df1.product_id',\n",
    "    fx.coalesce('df1.brand', 'df2.brand').alias('brand')\n",
    ")\n",
    "\n",
    "df_filled.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+------------+\n",
      "|brand|product_id|brand_filled|\n",
      "+-----+----------+------------+\n",
      "|    D|      1112|           D|\n",
      "|    A|      1234|           A|\n",
      "| NULL|      1234|           A|\n",
      "|    B|      5678|           B|\n",
      "| NULL|      5678|           B|\n",
      "|    C|      9101|           C|\n",
      "| NULL|      9101|           C|\n",
      "+-----+----------+------------+\n",
      "\n",
      "+----------+-----------+\n",
      "|product_id|brand_final|\n",
      "+----------+-----------+\n",
      "|      1112|          D|\n",
      "|      1234|          A|\n",
      "|      1234|          A|\n",
      "|      5678|          B|\n",
      "|      5678|          B|\n",
      "|      9101|          C|\n",
      "|      9101|          C|\n",
      "+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3 = df.withColumn(\n",
    "    \"brand_filled\",\n",
    "    fx.first(\"brand\", ignorenulls=True).over(Window.partitionBy(\"product_id\"))\n",
    ")\n",
    "df3.show()\n",
    "\n",
    "df4 = df3.withColumn(\n",
    "    \"brand_final\", fx.coalesce(\"brand\", \"brand_filled\")\n",
    ")\n",
    "\n",
    "df4.select('product_id','brand_final').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
