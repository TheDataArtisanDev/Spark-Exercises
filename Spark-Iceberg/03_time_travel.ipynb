{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"IcebergTimeTravel\") \\\n",
    "    .config(\"spark.sql.catalog.local\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.local.type\", \"hadoop\") \\\n",
    "    .config(\"spark.sql.catalog.local.warehouse\", \"/tmp/iceberg_warehouse\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.4.2\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+-------------------+---------+---------+--------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|committed_at           |snapshot_id        |parent_id|operation|manifest_list                                                                                                       |summary                                                                                                                                                                                                                                                                                         |\n",
      "+-----------------------+-------------------+---------+---------+--------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|2025-04-02 15:10:12.365|5568038035354584041|NULL     |append   |/tmp/iceberg_warehouse/db/sample_users/metadata/snap-5568038035354584041-1-b956f21e-4e06-4b80-b2f8-9e71d4e86774.avro|{spark.app.id -> local-1743586785589, added-data-files -> 2, added-records -> 2, added-files-size -> 1830, changed-partition-count -> 2, total-records -> 2, total-files-size -> 1830, total-data-files -> 2, total-delete-files -> 0, total-position-deletes -> 0, total-equality-deletes -> 0}|\n",
      "|2025-04-02 17:41:52.616|4271837051864160400|NULL     |append   |/tmp/iceberg_warehouse/db/sample_users/metadata/snap-4271837051864160400-1-55d2f395-a893-485f-8535-651d99ac97c0.avro|{spark.app.id -> local-1743595860290, added-data-files -> 6, added-records -> 6, added-files-size -> 5567, changed-partition-count -> 6, total-records -> 6, total-files-size -> 5567, total-data-files -> 6, total-delete-files -> 0, total-position-deletes -> 0, total-equality-deletes -> 0}|\n",
      "+-----------------------+-------------------+---------+---------+--------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inspect available snapshots\n",
    "snapshots_df = spark.sql(\"SELECT * FROM local.db.sample_users.snapshots\")\n",
    "snapshots_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 1: Using Snapshot ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-----------+\n",
      "|id |name|signup_date|\n",
      "+---+----+-----------+\n",
      "|2  |Alex|2024-02-01 |\n",
      "|1  |Soni|2024-01-01 |\n",
      "+---+----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use a valid snapshot ID obtained from above\n",
    "snapshot_id = \"5568038035354584041\"\n",
    "\n",
    "df_old = spark.read \\\n",
    "    .option(\"snapshot-id\", snapshot_id) \\\n",
    "    .format(\"iceberg\") \\\n",
    "    .load(\"local.db.sample_users\")\n",
    "\n",
    "df_old.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-----------+\n",
      "|id |name    |signup_date|\n",
      "+---+--------+-----------+\n",
      "|4  |Neo     |2024-04-01 |\n",
      "|3  |Morpheus|2024-03-01 |\n",
      "|2  |Alex    |2024-02-01 |\n",
      "|1  |Trinity |2024-01-01 |\n",
      "|5  |Smith   |2024-05-01 |\n",
      "|6  |Anderson|2024-06-01 |\n",
      "+---+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use a valid snapshot ID obtained from above\n",
    "snapshot_id = \"4271837051864160400\"\n",
    "\n",
    "df_new= spark.read \\\n",
    "    .option(\"snapshot-id\", snapshot_id) \\\n",
    "    .format(\"iceberg\") \\\n",
    "    .load(\"local.db.sample_users\")\n",
    "\n",
    "df_new.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-----------+\n",
      "|id |name    |signup_date|\n",
      "+---+--------+-----------+\n",
      "|4  |Neo     |2024-04-01 |\n",
      "|3  |Morpheus|2024-03-01 |\n",
      "|2  |Alex    |2024-02-01 |\n",
      "|1  |Trinity |2024-01-01 |\n",
      "|5  |Smith   |2024-05-01 |\n",
      "|6  |Anderson|2024-06-01 |\n",
      "+---+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "current_df = spark.read.table(\"local.db.sample_users\")\n",
    "current_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 2: Use the Timestamp (epoch millis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+-------------------+\n",
      "|committed_at           |snapshot_id        |\n",
      "+-----------------------+-------------------+\n",
      "|2025-04-02 15:10:12.365|5568038035354584041|\n",
      "|2025-04-02 17:41:52.616|4271837051864160400|\n",
      "|2025-04-02 17:52:13.098|560746950586654696 |\n",
      "|2025-04-02 18:40:03.527|2794978594534067236|\n",
      "|2025-04-02 18:40:35.269|349006547023142716 |\n",
      "+-----------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "snapshots_df = spark.sql(\"SELECT * FROM local.db.sample_users.snapshots\")\n",
    "snapshots_df.select(\"committed_at\", \"snapshot_id\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1748857212000\n",
      "+---+--------+-----------+----------------+\n",
      "| id|    name|signup_date|           email|\n",
      "+---+--------+-----------+----------------+\n",
      "|  2|    Alex| 2024-02-01|            NULL|\n",
      "|  1| Trinity| 2024-01-01|            NULL|\n",
      "|  3|    Nina| 2024-03-01|nina@example.com|\n",
      "|  4|     Neo| 2024-04-01|            NULL|\n",
      "|  3|Morpheus| 2024-03-01|            NULL|\n",
      "|  6|Anderson| 2024-06-01|            NULL|\n",
      "+---+--------+-----------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "# Convert to milliseconds since epoch\n",
    "ts = datetime.datetime(2025, 6, 2, 15, 10, 12, 0)\n",
    "epoch_ms = int(ts.timestamp() * 1000)\n",
    "print(epoch_ms)  # Prints something like this 1743904212365\n",
    "\n",
    "df_old = spark.read \\\n",
    "    .option(\"as-of-timestamp\", epoch_ms) \\\n",
    "    .format(\"iceberg\") \\\n",
    "    .load(\"local.db.sample_users\")\n",
    "\n",
    "df_old.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
