{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"IcebergTimeTravel\") \\\n",
    "    .config(\"spark.sql.catalog.local\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.local.type\", \"hadoop\") \\\n",
    "    .config(\"spark.sql.catalog.local.warehouse\", \"/tmp/iceberg_warehouse\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.4.2\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add a new column to the table\n",
    "spark.sql(\"ALTER TABLE local.db.sample_users ADD COLUMNS (email STRING)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "\n",
    "# Insert new data that includes the new column\n",
    "new_data = [(3, \"Nina\", date(2024, 3, 1), \"nina@example.com\")]\n",
    "df_new = spark.createDataFrame(new_data, [\"id\", \"name\", \"signup_date\", \"email\"])\n",
    "\n",
    "df_new.writeTo(\"local.db.sample_users\").append()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-----------+----------------+\n",
      "|id |name    |signup_date|email           |\n",
      "+---+--------+-----------+----------------+\n",
      "|4  |Neo     |2024-04-01 |NULL            |\n",
      "|3  |Nina    |2024-03-01 |nina@example.com|\n",
      "|3  |Morpheus|2024-03-01 |NULL            |\n",
      "|2  |Alex    |2024-02-01 |NULL            |\n",
      "|1  |Trinity |2024-01-01 |NULL            |\n",
      "|5  |Smith   |2024-05-01 |NULL            |\n",
      "|6  |Anderson|2024-06-01 |NULL            |\n",
      "+---+--------+-----------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read full table â€” older rows will have nulls for 'email'\n",
    "df = spark.read.table(\"local.db.sample_users\")\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
